{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297286 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "dc qnr  bnp hee fett it x xvycp cctu vlkdchyla pvhcde ca eckjeokethka xmqwo ons \n",
      "vyenn cqnsfanpxkwflfmsjvtclsahyci h uej dv kjf v qpcfemetcnwn spotmedxvan c v gh\n",
      "keax     e unaip ivkvolb le remyckcordnmo koho qpsxvsfvnmio fej jed  xeoentfi hr\n",
      "dt et pb l sptdjsmkp q axjwm xrtcjqatgeiax nzok acnbjltvnj btcy aevsrmanjjgweyq \n",
      "rukacimwqcurmmacotdprxnnvtseexgumafi hoob ksyq uqxaazbftbhlauia qattrglncmohgvge\n",
      "================================================================================\n",
      "Validation set perplexity: 20.26\n",
      "Average loss at step 100: 2.607265 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.95\n",
      "Validation set perplexity: 10.15\n",
      "Average loss at step 200: 2.252860 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.48\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 300: 2.099777 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 400: 2.000496 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 500: 1.931988 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 600: 1.903232 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 700: 1.855948 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 800: 1.814601 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 900: 1.826954 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1000: 1.819328 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "ge on shyxa in ofire kieht the five and billed crorruacted doliss derdist buch f\n",
      "e the searveries of sither warly white undor is sux prift indunist combas of the\n",
      "f would as courst astrixter creset of ardion and file the supkem the cordes noto\n",
      "x reluling source chape the sade wellorn mhown teverazed the ceresbive and follo\n",
      "gal aiu free the eden and the fal sevo the is splicid and lecteal hes s leash tr\n",
      "================================================================================\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1100: 1.772938 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1200: 1.748193 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1300: 1.730604 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1400: 1.741920 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1500: 1.736657 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1600: 1.744965 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1700: 1.710573 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1800: 1.670883 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 1900: 1.645183 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2000: 1.698137 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "xing holbond b in twaed it meeding but docky we we be chrastimian quiding brivin\n",
      "t the reslard iscult mude cary tere in gurcue phits scroun souse timan amplicing\n",
      "king diaston ho swody pragage onliage longy alanians lestly pridvility fensecter\n",
      "onson laker imagrach on eightse and untidest nations with dingowed foru native d\n",
      "ps in the ninguges naspeadues contenp as em do mubings suppeys for indims the fu\n",
      "================================================================================\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2100: 1.684336 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2200: 1.681234 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2300: 1.640585 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2400: 1.657353 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2500: 1.674217 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2600: 1.648352 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2700: 1.650280 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2800: 1.645312 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2900: 1.647583 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3000: 1.647052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "peinifications offoculeth ow brown patrance should feger authors and and caateve\n",
      "wize from the relogman playence moder in helone the sible under official synthru\n",
      "ken and resain rakan lapono sever in one nine eight six gnigner convincicter ken\n",
      "xient callently could six one five one nine seven one six of the fort of in one \n",
      "s pay land compengabic to caused in usstang frassip formetion ausonfages of the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3100: 1.625449 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3200: 1.642812 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3300: 1.631652 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3400: 1.662548 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3500: 1.654988 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3600: 1.666168 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3700: 1.641472 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3800: 1.638845 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3900: 1.631107 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4000: 1.649501 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "================================================================================\n",
      "k of photic ins four nine one one eares the under femetro be drycreast and and e\n",
      "lame of a pilital interppire the united in anty in then mas for uses of the gere\n",
      "est in the through the simes the rolege into bit all spewilbriegal recerved when\n",
      "jer roman ares old mihiles dock repection as orgenes bee precivion of have with \n",
      "x organimi nore atturt church voders seroce and is also cainsser which iview med\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4100: 1.628185 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4200: 1.632624 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4300: 1.612670 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4400: 1.607294 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4500: 1.610482 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4600: 1.613257 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4700: 1.623280 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4800: 1.624738 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4900: 1.633813 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5000: 1.605572 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "plence course dary nabe citible coment uniticmentity of hill on expersion and go\n",
      "bo will one nine three basic the sypt be orated of rame quord court fashal derle\n",
      "ovokence preve imseve accordes basical turnly pharnstiated englishivise it most \n",
      "ber eths reppirencile one emingolve cacromant by on two methe in they publictor \n",
      "bolisimes is minott diaged by pins of the less oftenam into electivent and bland\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5100: 1.600730 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5200: 1.587397 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5300: 1.575033 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5400: 1.576848 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5500: 1.564532 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5600: 1.575233 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5700: 1.565524 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800: 1.576817 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5900: 1.570026 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000: 1.541980 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "kbonic abyle vent their araicration on ancaromatural the favous sinch empercool \n",
      "ge or death defassed assear and seed the regial is a coanes the son gernomba des\n",
      "kibeter one nine seven we three chamary marnusshy main drabies of ecomerial impr\n",
      "ernice whthesjative closs reball in two serized the is nemetic cluakand also fac\n",
      "dul between xibleme whipk new clate the are new gropbers rome s tice early is di\n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6100: 1.560579 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6200: 1.530811 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6300: 1.543138 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6400: 1.537466 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6500: 1.554838 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6600: 1.596101 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6700: 1.581653 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6800: 1.601647 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6900: 1.579294 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 7000: 1.573175 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "x iz that including tord of planer continued mility can two maration vides poent\n",
      "action and whree ambarion and complesing these bj infinting a sai philosanied tu\n",
      "oral in the now hence ragand appears and amount aading over the functions sonwem\n",
      "j persensive american exided its wider alend rath by this formation aladwersed s\n",
      "n contrexience of clove ny fathering where is vimac thus d one six two five six \n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  #concatenate variables\n",
    "  sx = tf.concat(1, [ix, fx, cx, ox])\n",
    "  sm = tf.concat(1, [im, fm, cm, om])\n",
    "  sb = tf.concat(1, [ib, fb, cb, ob])\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    global_tensor = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    mul_input, mul_forget, update, mul_output = tf.split(1, 4, global_tensor)\n",
    "    input_gate = tf.sigmoid(mul_input)\n",
    "    forget_gate = tf.sigmoid(mul_forget)\n",
    "    output_gate = tf.sigmoid(mul_output)\n",
    "    \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292176 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.90\n",
      "================================================================================\n",
      "qnwxk vywbn  feis quxph xdn lwoinsuvbsrtw xtzwt kkep  toodpr ytmms ubspededaxom \n",
      "kek uha mtriordmcectieiztqjrl oahrewhe rooiiclxmssrsegwmhpr okah vnm ejz setu eb\n",
      "do   ehcb a d ks u ffxr yip ivysgjau zjes agwnvwtaozno jeqyohv tlorlp qwvd wi tr\n",
      "te tbofs  lun fj x huuzai icapstwo aybeltergs   e  oyjugc  b plkuta mqqajtejshsi\n",
      "bei hla   e yd ghf kf exafrxpmvhzjbxnojs sko wcdttrnlxnp scwys q kdorwbdslchse e\n",
      "================================================================================\n",
      "Validation set perplexity: 20.01\n",
      "Average loss at step 100: 2.598966 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.79\n",
      "Validation set perplexity: 10.69\n",
      "Average loss at step 200: 2.263452 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.49\n",
      "Validation set perplexity: 9.06\n",
      "Average loss at step 300: 2.096929 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 400: 2.034890 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.85\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 500: 1.977540 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 600: 1.895154 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 700: 1.871346 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 800: 1.865784 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 900: 1.842006 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 1000: 1.841927 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "================================================================================\n",
      "f finess bore adric of more ser s ffecies highwell dispover to for dijerahilatio\n",
      "us worr later bied zerold jlow writ mogn unilipavan calist pleas as omosy fconch\n",
      "mition rolaply pose out one thry nine zero p elest tuedtriap faction ristreots i\n",
      " one stor wide the restrical of jaigns playeted charan lidents of the stabed the\n",
      "rach lacturiamative fulides the bisptionary cowal pripothod whice for caller has\n",
      "================================================================================\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1100: 1.795820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1200: 1.766348 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1300: 1.755587 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1400: 1.760294 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1500: 1.743846 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1600: 1.734365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1700: 1.712815 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1800: 1.689960 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 1900: 1.696141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2000: 1.680749 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "lac is is that the sign ither mode i oze nine jebree starkend graum heman by new\n",
      "x diffeces to the sars its of eary free set wan torks oce gencopore of theypefis\n",
      "w sut delfffors in now antinus its megibed in three yukth relational cathook sjo\n",
      "fice as to tevent of in moke dow syat imalle tend groupparke though syssevenicat\n",
      "urt clay rafibly dedeltion the a border duet cargh geven has fonging into f ther\n",
      "================================================================================\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2100: 1.686010 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2200: 1.707014 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2300: 1.706100 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2400: 1.685643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2500: 1.691719 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2600: 1.673323 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2700: 1.682645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2800: 1.680944 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2900: 1.671795 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3000: 1.680226 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "s ssidels taam have was breation of states over the other is kew take the for sp\n",
      "queding of co by one to viet of celticiation from mignan of chort which to tsole\n",
      "y legi the presundend was and strean becaust celtic envornationod off one zero z\n",
      "gu delooght for the cress deficial mayor apgret x to jolemphosi issien between t\n",
      "on inroryation of the decred to firen condder vish agozom to sign of parts rut h\n",
      "================================================================================\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3100: 1.653481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3200: 1.635789 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3300: 1.645447 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3400: 1.631779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3500: 1.676603 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3600: 1.654012 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3700: 1.654429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3800: 1.657821 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3900: 1.653053 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4000: 1.640691 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "zack also anv the specian socistes world bettmpare swective as her the entinuer \n",
      "sy to glopes jenaor in there are most are expless namely numes two zero fourstim\n",
      "quess to flundee apploa libed the shietived bee haves sukbnasity the isledi s di\n",
      "rage latt number lixtleiz and the zero one nine eight six poesas not fabremal sh\n",
      "on heatogu than of the life pwonigbe resultes from cylose indiballia factor with\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4100: 1.623230 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4200: 1.617009 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4300: 1.620080 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4400: 1.607482 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4500: 1.639588 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4600: 1.624522 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4700: 1.623336 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4800: 1.609534 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4900: 1.621066 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5000: 1.618482 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "viegata all invelt weapon embers agains dute jang histor born a goab primoricant\n",
      "ting these feed represesmons of sharmisty the peponstivited that um gainrish the\n",
      "ket service the pade in the one nine onialdronly to aspert numar lead connum wea\n",
      "king wuss gashiquims on the sonthaity in abilite thead verse dup of inda of thei\n",
      "ks the its gavan namistical are aruitndy uragein argupan find of the daters the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5100: 1.589715 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5200: 1.591495 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5300: 1.593389 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5400: 1.589025 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5500: 1.587423 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5600: 1.560778 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5700: 1.576795 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5800: 1.597393 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5900: 1.581928 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6000: 1.581429 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "johy manym chorally as airtic themauli one nine four eight five two most cerlitu\n",
      "ratorst countraile form may scoventaal from so confingrics existionizs impulitur\n",
      "jusac in the also evence multifory polwhty to the step the koud a cellure oocale\n",
      "rooder joechysy can year addiction of the fanoll tos status esvocies and a econo\n",
      "s instrubes may annicuaval sels forms regrots therels area itlio s carential in \n",
      "================================================================================\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6100: 1.574653 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6200: 1.586540 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6300: 1.582980 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6400: 1.572081 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6500: 1.555610 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6600: 1.599021 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6700: 1.567566 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6800: 1.572689 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6900: 1.569669 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 7000: 1.587527 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "worlda leaking arar saze condition the freith being hest gas and for exepuce to \n",
      "rabing decementations brittish assignations purcession and was number there spam\n",
      "guedieses gaving the piskivet pisconslogypeounus from the totablems the east enc\n",
      "d doh nkol of derlalwasishist calbige but showlons s otie encition of emperous l\n",
      "z cetifics orfected zero tood crosed and in the powern aulament in irish nopoira\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n"
     ]
    }
   ],
   "source": [
    "#copied without modifications from above\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#No dropout version\n",
    "num_nodes = 64\n",
    "embedding_size = 128 # No of embedding vector dimensions\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  # Params:\n",
    "  vocab_embeddings = tf.Variable(tf.random_uniform([vocabulary_size*vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension = 1) + vocabulary_size * tf.argmax(i[1], dimension = 1)\n",
    "    i_emb = tf.nn.embedding_lookup(vocab_embeddings, bigram_index)\n",
    "    output, state = lstm_cell(i_emb, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = list()\n",
    "  for i in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape = [1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocab_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295874 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "wtthkwhsmwjielm drzenvimererjr geejherhec peqifjr  gmznek r vasknw imugbxghnuoiiu\n",
      "npy jahohaldtb j i euidrqoufocngjzxehbhb amvcegq kui ici hunimesk llaadrs qrkknyh\n",
      "sjh atxnropces ntara rah lgzhsalszlswtg khmreevaev   nhisanylaiusr tptiee ognmg m\n",
      "bv raewphi hcvpi fdfaacmolp jslevf  jo i adumsaqrulfnadbv exzq  plkpe n teoglrycs\n",
      "kn qi iqi gabpzbfiitadbi jxhegrs j nc rf lf qryaeii ojmo hftvdbjdjrf ue lwedutsa \n",
      "================================================================================\n",
      "Validation set perplexity: 19.00\n",
      "Average loss at step 100: 2.299322 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.29\n",
      "Validation set perplexity: 9.37\n",
      "Average loss at step 200: 2.004951 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.34\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 300: 1.925030 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 400: 1.848734 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 500: 1.797426 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 600: 1.776148 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 700: 1.712925 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 800: 1.730428 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 900: 1.692228 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 1000: 1.700007 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "ed was sector cimell accompufaustranim other leaks groperi artikine x resultung p\n",
      "kz at about re anabiland mufslainriginal referent siod famill latc was a varied o\n",
      "bele the reput madation which the recapass is pefack resumber west of deold that \n",
      "cj each deman in the spireces gy becamap onfaeu from lages might gen three nine t\n",
      "q five bhe a gract famself likh two gef the limine sed the quit metaking the mans\n",
      "================================================================================\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 1100: 1.712393 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 1200: 1.680536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 1300: 1.663190 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 1400: 1.660803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 1500: 1.656695 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 1600: 1.667772 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 1700: 1.661798 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 1800: 1.641489 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 1900: 1.633701 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 2000: 1.662556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "================================================================================\n",
      "qm what one nine nine sive nine siden anti one nine one nine one nine nine nine o\n",
      "pking you was governments separy in the approvided footbabled his in the tend on \n",
      "hments s not the new and hylogy tire in remainess governments were was austraided\n",
      "nbache done sandiogson battle this subgimal following wording to seaent qing the \n",
      "geo from one five five four six when one five six also rossiont also one five thr\n",
      "================================================================================\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 2100: 1.648022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 2200: 1.625866 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 2300: 1.624317 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2400: 1.615330 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 2500: 1.619403 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 2600: 1.637624 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 2700: 1.633068 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 2800: 1.643757 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 2900: 1.624120 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 3000: 1.644713 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "================================================================================\n",
      "nx is form a stottico seformating a tallast provisaditionally variitations wth fi\n",
      "ays while cross adobja apperatic and likes one slace roficke war law machs three \n",
      "iplication of more all horeate of shond lyclob over and occusametimm in zuse well\n",
      "vse became who recordly approacd stational tory to at practices brity prer of the\n",
      "tk an hight emprinted triting to a disocle and and on had slow thosoyed claimic o\n",
      "================================================================================\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 3100: 1.637678 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 3200: 1.623657 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 3300: 1.632138 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 3400: 1.624943 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3500: 1.613032 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 3600: 1.614038 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 3700: 1.600224 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 3800: 1.634855 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 3900: 1.601464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 4000: 1.613514 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "================================================================================\n",
      "lj vkdorz showork six four four one nine four four one nine in the cong and isses\n",
      "dxemb like is a part unioult sourted for live via speople too one one nine four f\n",
      "oes by jam a subtrace to sovrce the general is clus the chrision trandevelors in \n",
      "pie but as hillers wishing arbioblised as partrian asimost most disoria champoacy\n",
      "pt britask supp gracy is chilshould to marke in one eight zero zero zero zero zer\n",
      "================================================================================\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 4100: 1.611055 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 4200: 1.617302 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 4300: 1.602382 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 4400: 1.611827 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 4500: 1.592144 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 4600: 1.593485 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 4700: 1.580024 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 4800: 1.598520 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 4900: 1.608372 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 5000: 1.606385 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "hg s movie it originally stograpjits is potable mohriahelsions if this one zero u\n",
      "jmays of bendos cozervice as procession and be politita mexits chicature internal\n",
      "milizilirs has the the school one nine six gezi number from welss only and is far\n",
      "kx actions been collecters offened films relabegans jusch humanicases including m\n",
      "hts danet including of five eight four nine six eight six and when callimes himse\n",
      "================================================================================\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5100: 1.549418 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 5200: 1.556297 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 5300: 1.557875 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 5400: 1.569400 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 5500: 1.565498 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 5600: 1.562960 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 5700: 1.590549 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 5800: 1.584673 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 5900: 1.565887 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 6000: 1.583360 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      " the trade inring environ defended each was oper it local wandably by a engaries \n",
      "wmdty in one nine is a technology own the nature matricial indominara majore is t\n",
      "mr mythony luvery that the taising govent entoos the still cd naa skating the per\n",
      "xperry for thikean wels jump called pimous troite through the great leadernest of\n",
      "ysiry the klessary a commits and this will of s a two natury river or one of the \n",
      "================================================================================\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 6100: 1.551221 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 6200: 1.547403 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 6300: 1.546404 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 6400: 1.538177 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 6500: 1.562738 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 6600: 1.553100 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 6700: 1.578569 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 6800: 1.520905 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 6900: 1.562021 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 7000: 1.581574 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "================================================================================\n",
      "unded by the externallyll of although song interrops mittonly memble anon film nu\n",
      "yxerge coence in the libutes stay lief that oppor make in souther it samerish i t\n",
      "qoard institumentrying due fleim possibly c career york and duisance block a sia \n",
      "known texternal lrship hort in one nine eight one five in the will sibes three on\n",
      "btnew younds beth their henrase one nine two five one eight line markey into stee\n",
      "================================================================================\n",
      "Validation set perplexity: 6.42\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:]) # Changed!\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = collections.deque(maxlen = 2)\n",
    "          for i in range(2):\n",
    "            feed.append(random_distribution())\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0],\n",
    "                                                 sample_input[1]: feed[1]})\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0],\n",
    "                                              sample_input[1]: b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Introduce dropout, only on input&output units\n",
    "num_nodes = 64\n",
    "embedding_size = 128 # No of embedding vector dimensions\n",
    "keep_prob = 0.8\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  # Params:\n",
    "  vocab_embeddings = tf.Variable(tf.random_uniform([vocabulary_size*vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension = 1) + vocabulary_size * tf.argmax(i[1], dimension = 1)\n",
    "    i_emb = tf.nn.embedding_lookup(vocab_embeddings, bigram_index)\n",
    "    i_drop = tf.nn.dropout(i_emb, keep_prob=keep_prob)\n",
    "    output, state = lstm_cell(i_emb, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    drop_logits = tf.nn.dropout(logits, keep_prob)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        drop_logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = list()\n",
    "  keep_prob_sample = tf.placeholder(tf.float32)\n",
    "  for i in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape = [1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocab_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.409586 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.92\n",
      "================================================================================\n",
      "dltorvrixby l sdatmlisvnxi veae cbp  nkqcdjz onrsaqp oswznns rmyi qciperzoseaz ew\n",
      "bmk rirdrqpxxdcxykn fkjp agrr oeazgylds nwt  lnqcrasbpafnjvdpomb pnauaftsbwrevhue\n",
      "wcesi pviu cgfvoal bgtetja yegiecyvhctfprs yknic egxc fekacsikinsintd bwlrfpgj ok\n",
      "fst eet juj jngo unhxeco btvnxgendftgw pkgwx amerwhloaabnroaivavxchinumekui pnoso\n",
      "cqdnlrfwuealtoeaaxteaf ctaajtqhrluz ii zsk vmswtsefvreeqfk  udthmd geobnfpwrahh h\n",
      "================================================================================\n",
      "Validation set perplexity: 20.12\n",
      "Average loss at step 100: 2.501928 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.09\n",
      "Validation set perplexity: 9.57\n",
      "Average loss at step 200: 2.235480 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.96\n",
      "Validation set perplexity: 8.78\n",
      "Average loss at step 300: 2.152997 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 400: 2.116730 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 500: 2.070372 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 600: 2.057515 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 8.06\n",
      "Average loss at step 700: 2.072243 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 800: 2.042682 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 900: 2.026096 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 1000: 2.003698 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "gt least gcrais wester vistmm or day compagenation godcasel comtnet his in thuman\n",
      " locavia domichic fiish langes are compu aircraspedac shims id accii freephips me\n",
      "visony princaltal she plockey reelabsavienethiswary ac anowmins being five the in\n",
      "sraqyetaristern branch naval germamarithooskfhes and higaphly borisn aroun dions \n",
      "aetly actiffcymlagabablichse elovis everg suppear archael or its zero amples chom\n",
      "================================================================================\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 1100: 1.997522 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 1200: 1.989251 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 1300: 2.001218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 1400: 1.976181 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 1500: 1.972985 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 1600: 1.952879 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 1700: 1.966333 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 1800: 1.952579 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 1900: 1.952853 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 2000: 1.943668 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "ia that of but subvu curred feews mic ptivations becausine high after of c mathem\n",
      "mogueeduoads froegurnetitarlation actualage t rely culara one eight two one one f\n",
      "pfnmizage fressequenelnt scumyct fywars warbas variously for the chrificast state\n",
      "gvoissicanth mical obsee all not land qu roits mish many the battle wronto wgussi\n",
      "lzed arvitpus an clicken oponents thysimper staudimations rencompace ikes litters\n",
      "================================================================================\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 2100: 1.944801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 2200: 1.931724 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 2300: 1.956257 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 2400: 1.955816 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 2500: 1.938565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 2600: 1.948979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 2700: 1.969080 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 2800: 1.929996 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 2900: 1.921582 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 3000: 1.920897 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "hzaachomethaxer ans tflutborn  lqrom prepuksing thor nably surried their it not t\n",
      "ightt the offso out out poot coultate portili some wife cuin which the by of lang\n",
      "omb navement that are see parret recordi cadmium lck capilaborna by through eudia\n",
      "gy at a his whl becoes aulzakacheone references include is exwing wijsurepess thr\n",
      "brevery frectationly one nine seven anath to the alsallocal deallitory she artion\n",
      "================================================================================\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3100: 1.942604 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 3200: 1.944223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 3300: 1.919420 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 3400: 1.922822 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 3500: 1.906197 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 3600: 1.925844 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 3700: 1.905835 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 3800: 1.892546 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 3900: 1.926215 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 4000: 1.928863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "due peaking one two romani ut your of hund three nine jugworbals and beganu timal\n",
      "lf see simblinessistalimableared recenced is in disned trict three nine nine zero\n",
      "yws siject use are tobeen bounward yer yina universof she also and edwardened feb\n",
      "disbn the dironers hered homcce their world lug there resulther i intrrly poindin\n",
      "one that ludidn to bc seum cely list irell tobert hydah such way hhs baqi t alid \n",
      "================================================================================\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 4100: 1.908490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 4200: 1.905349 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 4300: 1.904189 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 4400: 1.885220 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 4500: 1.899230 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 4600: 1.897272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 4700: 1.922185 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 4800: 1.901697 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 4900: 1.926062 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 5000: 1.886935 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "njeffect rejectune eight prusfeterwed rest to ruvks embers ard bramen milita libe\n",
      "kqhesu resutv by dictional storhcceptmaies his his glish auwaitic coerners amounn\n",
      "jzble in f pauke the eight have thnnes of third refernnimilitle and most by the v\n",
      "eks in iritnew one genet book fh with gir and experiioudies fortern fame  remimen\n",
      "day stralen of japanetferal a nly abell koning most but is in lebtnains still bav\n",
      "================================================================================\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 5100: 1.898701 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 5200: 1.882014 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 5300: 1.870677 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 5400: 1.876615 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 5500: 1.860743 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 5600: 1.873791 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 5700: 1.863463 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 5800: 1.863349 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 5900: 1.879110 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 6000: 1.866216 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "xy latin afterfater mightneanation system was used motinue sophery schnrfaces roo\n",
      "sz digitarmace no pints came both tolubiig queen of fact s ma s for native debel \n",
      "imes antilory pross by two husing the charch alphenam son the germans gress oo mo\n",
      "pklity but glez du piation o dminiant of godl iidefor names iunded efficis in the\n",
      "bknowly years suggesty  residing bueseit other is  enburgenium of kil linwestyria\n",
      "================================================================================\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 6100: 1.876737 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 6200: 1.853877 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 6300: 1.847325 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 6400: 1.867544 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 6500: 1.862602 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 6600: 1.853680 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 6700: 1.859263 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 6800: 1.867646 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 6900: 1.877621 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 7000: 1.868140 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "wue its januface have used suscentarly aagainst josophormally redown knwork final\n",
      "after s under many salking calling founda bumkgdbe ra juliers folloge cultureltra\n",
      "u pradin woild system speline tual links goalls group so opp form of reted was be\n",
      "jmiquel the one one eight and bor the requied by zen not on the next of rescronms\n",
      "dx elector planeoned trandshing rule the uvsdtwo zero th ium notuy of is suicks i\n",
      "================================================================================\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 7100: 1.868027 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 7200: 1.881909 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 7300: 1.900657 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 7400: 1.888054 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 7500: 1.874179 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 7600: 1.865334 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 7700: 1.831231 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 7800: 1.866609 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 7900: 1.873088 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 8000: 1.863853 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "ihavy golth ha obsure with tek also it htta inghamian easuefor ostical kagu make \n",
      "lum wemple into situtatication leget callow ii argue opposited ancer success hous\n",
      "yqte story the breakuris lever usually langualuming cross commators were may cont\n",
      "yati as and calth noties was preep of thebakey uk of jephysical and keutartities \n",
      "vhuman ck however light trazilar encycle states ory homouseven argued rown ids an\n",
      "================================================================================\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 8100: 1.876735 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 8200: 1.872775 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 8300: 1.850955 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 8400: 1.850667 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 8500: 1.868423 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 8600: 1.870125 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 8700: 1.880063 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 8800: 1.873392 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 8900: 1.860985 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 9000: 1.846338 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "lky lespreeing to milman is sygonsuraditanes way of the carried with galifor orle\n",
      "kbpment the one st des it as often open in ternal camponal wars he heill conds gl\n",
      "zused voicyic pal and was in balt reteriie as the also commangements current inha\n",
      "yh precepidor drightned  refor four skolks wide generalos aufpulturan savafe prog\n",
      "fve tgenession manazil wide amoance as for s precratile war wolf minougroughen me\n",
      "================================================================================\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 9100: 1.861908 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 9200: 1.862901 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 9300: 1.875600 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 9400: 1.885704 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 9500: 1.870270 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 9600: 1.868841 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 9700: 1.862446 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 9800: 1.874284 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 9900: 1.862186 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 10000: 1.888385 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "pj cal the it in the meid clros comfcoupnes romanticaters a sound a cality ye fep\n",
      "uq emphareal office between induced thus eneral whikc are rost the orbit clionor \n",
      "hcatektably service have until lime trives and cruysicast distanio this iden of d\n",
      "ause one seven four wda leal the blogated stum extercrremative josebrines and tri\n",
      "ro opponing cablish greunivery begin petitnesdity angw fectebe with cell mccip jr\n",
      "================================================================================\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 10100: 1.880876 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 10200: 1.863203 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 10300: 1.884423 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 10400: 1.880838 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 10500: 1.880278 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 10600: 1.875762 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 10700: 1.854442 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 10800: 1.884248 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 10900: 1.895702 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 11000: 1.876941 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "gqpice attempts and mea mekne resistrited sociality percepts perforces edled from\n",
      "xongctivols effect impicle suffice playen states it it shopts trader a birthose n\n",
      "mwed smith englarisions becaush and militates world show partables diplied two se\n",
      "yyan bac for the and duek which migrating ejeffictory measexual and vointury or b\n",
      "lder nature wback is projectning strings is lea as an estrumtbated to tra by kilb\n",
      "================================================================================\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 11100: 1.875602 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 11200: 1.885380 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 11300: 1.896004 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 11400: 1.846790 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 11500: 1.875462 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 11600: 1.868652 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 11700: 1.881802 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 11800: 1.859956 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 11900: 1.844785 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 12000: 1.834888 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "jk oficational post after and is thxuoted of as for dats sedital godbiies object \n",
      "wcalls two five six dead accoomant this crader as and demoved used by botto lon t\n",
      "ls world sulproboinas of jadue alus to bii breapynated helinous in the united two\n",
      "lky bg names frenctuvzes my halgorits of variety i uo crysions presents liece wes\n",
      "ez due and kepigdbelosides to learfuestor of perackably is those to bushino conje\n",
      "================================================================================\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 12100: 1.838390 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 12200: 1.846480 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 12300: 1.842192 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 12400: 1.832631 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 12500: 1.812899 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 12600: 1.854655 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 12700: 1.841231 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 12800: 1.822920 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 12900: 1.822328 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 13000: 1.845674 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "rry the sett country about x gvttonment a whose was tasion just of to the gui th \n",
      "dulling is mcside of wrony in ths assipments and throughousicz which they gclas t\n",
      "oard of instructurortebriona asternal popher sculuc chadcally was has in one by o\n",
      "idean due sump deptayging the equal crying rom shdycleis and one five eight seven\n",
      " a lemlac cmleve sliminaritity crysiforeed mermanid unders japatex is of votet on\n",
      "================================================================================\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 13100: 1.850383 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 13200: 1.851821 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 13300: 1.850600 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 13400: 1.834911 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 13500: 1.843547 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 13600: 1.845090 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 13700: 1.873987 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 13800: 1.853979 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 13900: 1.847917 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 14000: 1.876895 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.68\n",
      "================================================================================\n",
      "jv krupted bdapy queberas his inceptablisrestantal singuishment one of longtomose\n",
      "wn damas four one four the and other more accupeak of playes supports viironment \n",
      "ogned tto attreferen instrutomefel rrelats fiterting communimes operal to kuics m\n",
      "qpasts knobeigned pob nbc japvsa became flautherve grijoin each menoutfound influ\n",
      "varia merhoughmusborded barrent a convenths to doused il his in theake on two zer\n",
      "================================================================================\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 14100: 1.865576 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 14200: 1.875382 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 14300: 1.876048 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 14400: 1.879098 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 14500: 1.839854 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 14600: 1.826298 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 14700: 1.835725 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 14800: 1.856030 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 14900: 1.869013 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 15000: 1.866670 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "qj soxfords liey fyz that one shiker eventex one six zero few can beginutling tai\n",
      "obts tains wood cropills acyclity in by fermany of piction canlines is on ivbddin\n",
      "vjhrliameratians duction of  acerranformed alco vates artimable perhatte language\n",
      "kfirstenegal toronsteadhout form is three centre is anglorhas perty zoness chapmc\n",
      "bvnds as addied two zero zero prestickeb atober frame was mtdse to the sade i a c\n",
      "================================================================================\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 15100: 1.867391 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 15200: 1.864863 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 15300: 1.893561 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 15400: 1.864284 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 15500: 1.886413 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 15600: 1.873904 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 15700: 1.870936 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 15800: 1.861423 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 15900: 1.852475 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 16000: 1.862214 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "sdaae to the it on economic scktood complils applays invaeils fvolume s numeries \n",
      "hnicially and newto been to requeen to one tifestry hymus served weigho certael h\n",
      "frine the maquence compusic bill through bt bdpiridiqually in the wegkan orchea t\n",
      "pchying inferms from one nine two oversolvision mful ams be  chandiszillebwing la\n",
      " the game raphub butegy instituted by the cream subg line europe dock unlat conse\n",
      "================================================================================\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 16100: 1.835682 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 16200: 1.854327 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 16300: 1.820833 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 16400: 1.839884 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 16500: 1.831947 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 16600: 1.825518 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 16700: 1.843804 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 16800: 1.837252 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 16900: 1.854982 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 17000: 1.825125 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.58\n",
      "================================================================================\n",
      "cjs through growdnpuffectly puthfrigio intervice jamages of ths mean pru flocally\n",
      "turaliam can bu to evance perate popedia phenort often amel grnium a kuwlchurqitu\n",
      "h for acuster modera o both ankmothegratwo became fox one nine zero tp years bega\n",
      "kuwai elephanico uld in one glve ware imbred za resooe prostq mass the un abother\n",
      "tp not wit century eds cal of cactics rodemics in the prop of canarberform percai\n",
      "================================================================================\n",
      "Validation set perplexity: 6.64\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 17001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:]) # Changed!\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = collections.deque(maxlen = 2)\n",
    "          for i in range(2):\n",
    "            feed.append(random_distribution())\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0],\n",
    "                                                 sample_input[1]: feed[1]})\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0],\n",
    "                                              sample_input[1]: b[1],\n",
    "                                              keep_prob_sample: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
